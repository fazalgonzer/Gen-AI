{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepinfra.com/blog/function-calling-feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "User can supply any kind of functions to the model (aka. Assistant). The Assistant will be useful, as long as the descriptions of the functions are well-written.\n",
    "Cons:\n",
    "\n",
    "User (most likely Developer) needs to write some code, or have a way to execute the function with the parameters that the model decided to call.\n",
    "During the interaction the LLM model might ask clarifying questions in order to execute functions with the right arguments.\n",
    "\n",
    "Use cases\n",
    "Here are just a few example use cases that can benefit from Function Calling capability of LLMs:\n",
    "\n",
    "Real-time data processor: you can provide a function that accesses the real-time data and does some actions\n",
    "Math Problem Solver: it can be used to solve complex math problems (e.g. by providing a function that talks to WolframAlpha or other third-party tools)\n",
    "AI Virtual Assistants: you can enhance your AI assistant with various functionalities that the LLM model can call and respond back meaningfully to the end user\n",
    "Now, let's consider a concrete example.\n",
    "\n",
    "Example with code\n",
    "Let's consider this example:\n",
    "\n",
    "User has a function get_current_weather(location)\n",
    "User wants to find out the city with the hottest weather.\n",
    "Here is the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Which city has the hottest weather today: San Francisco, Tokyo, or Paris?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\":\n",
    "                        \"The city and state, e.g. San Francisco, CA\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        },\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\n\u001b[0;32m      4\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.deepinfra.com/v1/openai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<Your-DeepInfra-API-Key>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m     11\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m     12\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    api_key=\"<Your-DeepInfra-API-Key>\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "tool_calls = response.choices[0].message.tool_calls\n",
    "for tool_call in tool_calls:\n",
    "    print(tool_call.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    print(\"Calling get_current_weather client side.\")\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\n",
    "            \"location\": \"Tokyo\",\n",
    "            \"temperature\": \"75\"\n",
    "        })\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\n",
    "            \"location\": \"San Francisco\",\n",
    "            \"temperature\": \"60\"\n",
    "        })\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\n",
    "            \"location\": \"Paris\",\n",
    "            \"temperature\": \"70\"\n",
    "        })\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in tool_calls:\n",
    "    function_name = tool_call.function.name\n",
    "    if function_name == \"get_current_weather\":\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        function_response = get_current_weather(\n",
    "            location=function_args.get(\"location\")\n",
    "        )\n",
    "\n",
    "    # extend conversation with function response\n",
    "    messages.append({\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": function_response,\n",
    "    })\n",
    "\n",
    "\n",
    "# get a new response from the model where it can see the function responses\n",
    "second_response = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(second_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
